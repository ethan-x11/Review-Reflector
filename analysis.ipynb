{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/B09G9FPHY6_2023-12-01.csv')\n",
    "df[df['label']=='positive'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Score Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_score = df.loc[df['label'] == 'positive', ['date', 'score']]\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.lineplot(x='date', y='score', data=positive_score, color='green', errorbar=None,  linestyle='--', marker='o')\n",
    "sns.barplot(x='date', y='score', data=positive_score,errorbar=None, hue='date', legend=False, alpha=0.7, palette=sns.color_palette(\"crest\", n_colors=len(positive_score)))\n",
    "plt.xlabel('Date Range: ' + str(positive_score['date'].min()) + ' to ' + str(positive_score['date'].max()), fontsize=14, labelpad=10, color='green', fontweight='bold')\n",
    "plt.ylabel('Positivity',fontsize=14, labelpad=10, color='green', fontweight='bold')\n",
    "plt.title('Positive Score Graph', fontsize=20, color='green', fontweight='bold' )\n",
    "plt.xticks(rotation=90)\n",
    "plt.gca().set_xticklabels([])\n",
    "# plt.show()\n",
    "print(fig)\n",
    "l = list()\n",
    "l.append(fig)\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Score Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_score = df.loc[df['label'] == 'negative', ['date', 'score']]\n",
    "plt.figure(figsize=(20, 4))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.lineplot(x='date', y='score', data=negative_score, color='green', errorbar=None,  linestyle='--', marker='o')\n",
    "sns.barplot(x='date', y='score', data=negative_score,errorbar=None, hue='date', legend=False, alpha=0.7, palette=sns.color_palette(\"crest\", n_colors=len(negative_score)))\n",
    "plt.xlabel('Date Range: ' + str(negative_score['date'].min()) + ' to ' + str(negative_score['date'].max()), fontsize=14, labelpad=10, color='green', fontweight='bold')\n",
    "plt.ylabel('Negativity',fontsize=14, labelpad=10, color='green', fontweight='bold')\n",
    "plt.title('Negative Score Graph', fontsize=20, color='green', fontweight='bold' )\n",
    "plt.xticks(rotation=90)\n",
    "plt.gca().set_xticklabels([])\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig('analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating vs Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingVSdate = df[['date', 'rating']]\n",
    "plt.figure(figsize=(20, 4))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.lineplot(x='date', y='rating', data=ratingVSdate, color='green', errorbar=None, label='Line', linestyle='--', marker='o')\n",
    "# sns.barplot(x='date', y='rating', data=ratingVSdate,errorbar=None, hue='date', legend=False, alpha=0.7, palette=sns.color_palette(\"crest\", n_colors=len(ratingVSdate)//2))\n",
    "plt.xlabel('Time', fontsize=14, labelpad=10, color='green', fontweight='bold')\n",
    "plt.ylabel('Rating',fontsize=14, labelpad=10, color='green', fontweight='bold')\n",
    "plt.title('Rating VS Date', fontsize=20, color='green', fontweight='bold' )\n",
    "plt.xticks(rotation=90)\n",
    "plt.gca().set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_positive_review = df.loc[(df['label'] == 'positive') & (df['rating'] == 5.0)]\n",
    "Top_positive_review.sort_values(by='score', ascending=False, inplace=True)\n",
    "Top_positive_review = Top_positive_review [['date', 'title','text', 'location' ]]\n",
    "Top_positive_review.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_negative_review = df.loc[(df['label'] == 'negative') & (df['rating'] == 1.0)]\n",
    "Top_negative_review.sort_values(by='score', ascending=False, inplace=True)\n",
    "Top_negative_review = Top_negative_review [['date', 'title','text', 'location' ]]\n",
    "Top_negative_review.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critcal Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Critical_Positive = df.loc[(df['label'] == 'positive')]\n",
    "Critical_Positive.sort_values(by='score', ascending=False, inplace=True)\n",
    "Critical_Positive = Critical_Positive [['date', 'title','text', 'rating', 'location' ]]\n",
    "Critical_Positive.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Critical_Negative = df.loc[(df['label'] == 'negative')]\n",
    "Critical_Negative.sort_values(by='score', ascending=False, inplace=True)\n",
    "Critical_Negative = Critical_Negative [['date', 'title','text', 'rating', 'location' ]]\n",
    "Critical_Negative.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading opinion_lexicon: <urlopen error [WinError\n",
      "[nltk_data]     10060] A connection attempt failed because the\n",
      "[nltk_data]     connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('opinion_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = df.loc[df['label'] == 'positive', ['translated_text']]\n",
    "negative_reviews = df.loc[df['label'] == 'negative', ['translated_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\subha/nltk_data'\n    - 'c:\\\\Users\\\\subha\\\\Desktop\\\\Sentiment-Analysis\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\subha\\\\Desktop\\\\Sentiment-Analysis\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\subha\\\\Desktop\\\\Sentiment-Analysis\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\subha\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\subha\\Desktop\\Sentiment-Analysis\\venv\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\subha\\Desktop\\Sentiment-Analysis\\venv\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\subha/nltk_data'\n    - 'c:\\\\Users\\\\subha\\\\Desktop\\\\Sentiment-Analysis\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\subha\\\\Desktop\\\\Sentiment-Analysis\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\subha\\\\Desktop\\\\Sentiment-Analysis\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\subha\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\subha\\Desktop\\Sentiment-Analysis\\analysis.ipynb Cell 19\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/subha/Desktop/Sentiment-Analysis/analysis.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/subha/Desktop/Sentiment-Analysis/analysis.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m stemmer \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/subha/Desktop/Sentiment-Analysis/analysis.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m stop_words\u001b[39m.\u001b[39mupdate(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/subha/Desktop/Sentiment-Analysis/analysis.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m stemmer \u001b[39m=\u001b[39m SnowballStemmer(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/subha/Desktop/Sentiment-Analysis/analysis.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m filtered_words \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\subha\\Desktop\\Sentiment-Analysis\\venv\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\subha\\Desktop\\Sentiment-Analysis\\venv\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\subha\\Desktop\\Sentiment-Analysis\\venv\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\subha\\Desktop\\Sentiment-Analysis\\venv\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\subha/nltk_data'\n    - 'c:\\\\Users\\\\subha\\\\Desktop\\\\Sentiment-Analysis\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\subha\\\\Desktop\\\\Sentiment-Analysis\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\subha\\\\Desktop\\\\Sentiment-Analysis\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\subha\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "positive_reviews_list = positive_reviews['translated_text'].tolist()\n",
    "word= []\n",
    "for i in positive_reviews_list:\n",
    "    word.append(word_tokenize(i.lower()))\n",
    "word_list = [i for sublist in word for i in sublist]\n",
    "stop_words = set()\n",
    "stemmer = set()\n",
    "stop_words.update(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in word_list: \n",
    "    if word not in stop_words and word.isalpha():\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        filtered_words.append(stemmed_word)\n",
    "\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "\n",
    "positive_word_list = [word for word in filtered_words if word in positive_words]\n",
    "\n",
    "\n",
    "text = ' '.join(positive_word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud()\n",
    "wordcloud.max_font_size = 50\n",
    "wordcloud.max_words = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THUMBS_UP_FILE = './Images/thumbs-up.png'\n",
    "icon = Image.open(THUMBS_UP_FILE)\n",
    "image_mask = Image.new(mode='RGB', size = icon.size, color=(255,255,255))\n",
    "image_mask.paste(icon, box = icon)\n",
    "rgb_array = np.array(image_mask)\n",
    "\n",
    "word_cloud = WordCloud(mask = rgb_array, background_color='white',contour_width=1, contour_color='black',\n",
    "                      colormap='viridis')\n",
    "word_cloud.generate(text)\n",
    "\n",
    "plt.figure(figsize=([16, 8]))\n",
    "\n",
    "plt.imshow(word_cloud, interpolation='bicubic')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews_list = negative_reviews['translated_text'].tolist()\n",
    "word= []\n",
    "for i in negative_reviews_list:\n",
    "    word.append(word_tokenize(i.lower()))\n",
    "word_list = [i for sublist in word for i in sublist]\n",
    "stop_words = set()\n",
    "stemmer = set()\n",
    "stop_words.update(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in word_list: \n",
    "    if word not in stop_words and word.isalpha():\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        filtered_words.append(stemmed_word)\n",
    "\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "negative_word_list = [word for word in filtered_words if word in negative_words]\n",
    "\n",
    "text_negative = ' '.join(negative_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THUMBS_DOWN_FILE = './Images/thumbs-down.png'\n",
    "icon = Image.open(THUMBS_DOWN_FILE)\n",
    "image_mask = Image.new(mode='RGB', size = icon.size, color=(255,255,255))\n",
    "image_mask.paste(icon, box = icon)\n",
    "rgb_array = np.array(image_mask)\n",
    "\n",
    "word_cloud = WordCloud(mask = rgb_array, background_color='white',contour_width=1, contour_color='black',\n",
    "                      colormap='viridis')\n",
    "word_cloud.generate(text_negative)\n",
    "\n",
    "plt.figure(figsize=([16, 8]))\n",
    "\n",
    "plt.imshow(word_cloud, interpolation='bicubic')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
